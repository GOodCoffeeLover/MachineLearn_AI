{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde94b6a",
   "metadata": {},
   "source": [
    "# Лабораторная работа №2  \"Алгоритмы классификации\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc86dab",
   "metadata": {},
   "source": [
    "**ФИО**: *Трофимов М.А.*\n",
    "\n",
    "**Группа**: *М8О-308Б-18*\n",
    "\n",
    "**Номер по списку**: *24*\n",
    "\n",
    "**Вариант**: *24%2+1=1*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeae6b8",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab751393",
   "metadata": {},
   "source": [
    "Установка необходимых библиотек при необходимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873bd39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/schizophrenia/.local/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/schizophrenia/.local/lib/python3.8/site-packages (from pandas) (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/schizophrenia/.local/lib/python3.8/site-packages (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /home/schizophrenia/.local/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/schizophrenia/.local/lib/python3.8/site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/schizophrenia/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/schizophrenia/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/schizophrenia/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e77757",
   "metadata": {},
   "source": [
    "Загрузка и подкотовка датасета. Загружаем датасет, удаляем се строки содержащие неизвестные данные. Затем удаляем лишние столбцы, которые либо связанные с имеющимися, либо лишены смысла(fnlwgt). Также заменяем названия классов на 1 и 0, для дальнейшей удобной работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbac35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45217</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45218</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45219</th>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45220</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45221</th>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45222 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  workclass  educational-num  occupation  relationship  race  \\\n",
       "0       25          2                7           6             3     2   \n",
       "1       38          2                9           4             0     4   \n",
       "2       28          1               12          10             0     4   \n",
       "3       44          2               10           6             0     2   \n",
       "4       34          2                6           7             1     4   \n",
       "...    ...        ...              ...         ...           ...   ...   \n",
       "45217   27          2               12          12             5     4   \n",
       "45218   40          2                9           6             0     4   \n",
       "45219   58          2                9           0             4     4   \n",
       "45220   22          2                9           0             3     4   \n",
       "45221   52          3                9           3             5     4   \n",
       "\n",
       "       gender  hours-per-week  native-country income  \n",
       "0           1              40              38      1  \n",
       "1           1              50              38      1  \n",
       "2           1              40              38      0  \n",
       "3           1              40              38      0  \n",
       "4           1              30              38      1  \n",
       "...       ...             ...             ...    ...  \n",
       "45217       0              38              38      1  \n",
       "45218       1              40              38      0  \n",
       "45219       0              40              38      1  \n",
       "45220       1              20              38      1  \n",
       "45221       0              40              38      0  \n",
       "\n",
       "[45222 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./adult.csv\", na_values = \"?\")\n",
    "data = data.dropna()\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data=data.drop(labels=['education', 'capital-loss', 'capital-gain', 'marital-status', 'fnlwgt'], axis=1)\n",
    "\n",
    "categ_columns = ['workclass', 'occupation', 'relationship', 'race', 'gender', 'native-country']\n",
    "data[categ_columns] = data[categ_columns].apply(lambda col:pd.Categorical(col).codes)\n",
    "\n",
    "income = {'<=50K': 1, '>50K': 0}\n",
    "data['income'] = pd.Categorical(data['income']).rename_categories(income)    \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb696a",
   "metadata": {},
   "source": [
    "Теперь у нас все значения в таблице выражены числом.\n",
    "Теперь разобьём выборку на обучающую и тестовую в пропорции 2 к 1, приведём их к *numpy.array* для удобства работы с ними и разобъем отдельно на параметры(X) и метки класса(Y).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da74a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "shuffled = data.sample(frac=1)\n",
    "parts = np.array_split(shuffled, 3)\n",
    "data_learn = pd.concat(parts[0:2])\n",
    "data_test = parts[2]\n",
    "data_learn.reset_index(inplace=True, drop=True)\n",
    "data_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "np_data = data.to_numpy()\n",
    "np_data_learn = data_learn.to_numpy()\n",
    "np_data_test = data_test.to_numpy()\n",
    "\n",
    "X_l, Y_l = np_data_learn[:, :-1], np_data_learn[:, -1]\n",
    "X_t, Y_t = np_data_test[:, :-1], np_data_test[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae97611",
   "metadata": {},
   "source": [
    "# Логистическая Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94855a90",
   "metadata": {},
   "source": [
    "Идея логистической регресси в том, что мы хотим подобрать такие параметры $w, b$ для функции $ \\alpha(x,w, b) = \\left[\\frac{1}{1+\\exp^{-(w\\cdot x + b)}} >= 0.5\\right]$ давала наиболее точное предсказание $\\hat{y}$ для настоящих значений $y$. \n",
    "\n",
    "Иначе данную задачу можно поставить как задачу минимизации логорифмической функции потерь: $$J(w,b) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i\\cdot\\log(\\hat{y}) - (1-y_i)\\cdot\\log(1-\\hat{y})\\right]$$\n",
    "\n",
    "Данная задача имеет достаточно простой метод решения: градиентный спуск. В нашем случае производные в нашем градиенте будут численно равны:\n",
    "$$\\frac{\\partial L}{\\partial w_i} = \\frac{x_i \\bullet (\\hat{y} - y)}{N} \\text{и} \\frac{\\partial L}{\\partial b} = \\frac{(\\hat{y} - y)}{N}  $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4fdb7",
   "metadata": {},
   "source": [
    "Обучение происходит не сразу на всей выборке, а небольшими \"батчами\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d755a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1+np.exp(-z))\n",
    "\n",
    "def loss(y, y_pred):\n",
    "    loss = -np.mean(y*np.log(y_pred) - (1-y)*np.log(1-y_pred))\n",
    "    return loss\n",
    "\n",
    "def gradient(X, Y, y_pred):\n",
    "    m = X.shape[0]\n",
    "    dw = np.dot(X.T, (y_pred - Y))/m\n",
    "    db = np.sum(y_pred - Y)/m\n",
    "    return dw, db\n",
    "\n",
    "def normalize(X):\n",
    "    m,n = X.shape\n",
    "    \n",
    "    for i in range(n):\n",
    "        X[:, i] = (X[:, i] - X[:, i].mean())/X[:, i].std()\n",
    "    return X\n",
    "\n",
    "def train(X, Y, bs, epochs, lr):\n",
    "    m,n = X.shape\n",
    "    w = np.zeros((n,1)) \n",
    "    b=0\n",
    "    Y = Y.reshape(m,1)\n",
    "    X = normalize(X)\n",
    "    losses = []\n",
    "    for ep in range(epochs):\n",
    "        for i in range((m-1)//bs+1):\n",
    "            xb = X[i*bs:(i+1)*bs]\n",
    "            yb = Y[i*bs:(i+1)*bs]\n",
    "            \n",
    "            y_pred = sigmoid(np.dot(xb, w) +b)\n",
    "            \n",
    "            dw,db = gradient(xb, yb, y_pred)\n",
    "            \n",
    "            w-=lr*dw\n",
    "            b-=lr*db\n",
    "        losses.append(loss(Y, sigmoid(np.dot(X, w)+b))) \n",
    "    return w, b, losses\n",
    "\n",
    "def predict(X, w, b):\n",
    "    x = normalize(X)\n",
    "    pred = sigmoid(np.dot(X, w)+b)\n",
    "    \n",
    "    pred_classes = [1 if p>=0.5 else 0 for p in pred]\n",
    "    return np.array(pred_classes)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ce1030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7906992171951704\n",
      "precision: 0.8241827991113932\n",
      "recall:    0.9171007327624261\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "W, B, losses = train(X_l, Y_l, bs=500, epochs=100, lr=0.1)\n",
    "Y_pred = predict(X_t, W, B)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_t, Y_pred).ravel()\n",
    "\n",
    "acc:float = (tn + tp)/(tp+tn + fp + fn)\n",
    "prec:float = tp/(tp + fp)\n",
    "rec:float = tp/(tp + fn)\n",
    "print(\"accuracy:  {0}\\nprecision: {1}\\nrecall:    {2}\".format(acc, prec, rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "106a9506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7906992171951704\n",
      "precision: 0.8241313660161828\n",
      "recall:    0.917189017392072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "clf = LogisticRegression(random_state=0, max_iter=100000).fit(X_l, Y_l)\n",
    "Y_pred = clf.predict(X_t)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_t, Y_pred).ravel()\n",
    "\n",
    "acc:float = (tn + tp)/(tp+tn + fp + fn)\n",
    "prec:float = tp/(tp + fp)\n",
    "rec:float = tp/(tp + fn)\n",
    "print(\"accuracy:  {0}\\nprecision: {1}\\nrecall:    {2}\".format(acc, prec, rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e98c1",
   "metadata": {},
   "source": [
    "Как видим, получившаяся модель почти не уступает в точности библиотечной моделе sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1c045",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc86f94c",
   "metadata": {},
   "source": [
    "Основная идея **SVM** или Support Vector Machine состоит в том, что мы хотим найти такую гиперплоскость $w\\cdot x + b =0$, чтобы она разделяла пространство на две части. По одну сторону от неё был положительный класс, а по другую - отрицательный, а расстояние от неё, до ближайший представителей из каждого калсса было максимальным. Тогда нашу задачу можно сформулировать через минимизацию функции потерь Хинджи с применением к ней L2 регулезации: $$ L(w, b) = \\lambda\\| w \\|^2 + \\frac{1}{N} \\sum_{i=1}^{N} \\max(0, 1-y_i(w\\cdot x_i - b)) $$\n",
    "\n",
    "Или разбивая на два случая $$L_i(w,b) = \\left\\{ \\begin{array}{cc} \\lambda\\| w \\|^2, & y_i(w\\cdot x_i - b)\\geq 1\\\\ \\lambda\\| w \\|^2 + 1 - y_i(w\\cdot x_i - b), & else \\end{array}\\right. $$\n",
    "\n",
    "Оптимизацию так же можно делать через градиентный спуск. Тогда производные будут равны:\n",
    "$\\frac{\\partial L_i(w,b)}{\\partial w_j} = \\left\\{ \\begin{array}{cc} 2\\lambda\\cdot w_j, & y_i(w\\cdot x_{i} - b)\\geq 1\\\\ 2\\lambda \\cdot w_j - y_i\\cdot x_{ij}, & else \\end{array}\\right. $ и $\\frac{\\partial L_i(w,b)}{\\partial b} = \\left\\{ \\begin{array}{cc} 0, & y_i(w\\cdot x_i - b)\\geq 1\\\\  y_i, & else \\end{array}\\right. $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b9ac887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train(X, Y, lr=0.00001, lam=0.001, epochs=100):\n",
    "    n, m = X.shape\n",
    "    y_ = np.where(Y<=0, -1, 1)\n",
    "    w = np.zeros(m).astype('float128')\n",
    "    b=0\n",
    "    for _ in range(epochs):\n",
    "        for i in range(n):\n",
    "            if y_[i] * (np.dot(X[i], w)-b) >=1 :\n",
    "                w -= lr*(2*lam*w)\n",
    "                #b -= lr*y_[i]\n",
    "            else:\n",
    "                w -= lr*(2*lam*w - np.dot(X[i], y_[i]))\n",
    "                b -= lr*y_[i]\n",
    "    return w, b\n",
    "    \n",
    "def svm_predict(X, w, b, rerange:bool=True):\n",
    "    #X = normalize(X_)\n",
    "    classes = np.sign(np.dot(X, w)-b)\n",
    "    return np.where(classes<=0, 0, 1) if rerange else classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "559a545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7543452301976914\n",
      "precision: 0.7542689434364994\n",
      "recall:    0.9983225920367264\n"
     ]
    }
   ],
   "source": [
    "w, b = svm_train(X_l, Y_l)\n",
    "\n",
    "Y_pred = svm_predict(X_t, w, b)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_t, Y_pred).ravel()\n",
    "\n",
    "acc:float  = (tn + tp)/(tp+tn + fp + fn)\n",
    "prec:float = tp /(tp + fp)\n",
    "rec:float  = tp /(tp + fn)\n",
    "print(\"accuracy:  {0}\\nprecision: {1}\\nrecall:    {2}\".format(acc, prec, rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d893b5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7993233381982221\n",
      "precision: 0.8290266328471781\n",
      "recall:    0.9233689414672905\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_l, Y_l)\n",
    "Y_pred = clf.predict(X_t)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_t, Y_pred).ravel()\n",
    "\n",
    "acc:float = (tn + tp)/(tp+tn + fp + fn)\n",
    "prec:float = tp/(tp + fp)\n",
    "rec:float = tp/(tp + fn)\n",
    "print(\"accuracy:  {0}\\nprecision: {1}\\nrecall:    {2}\".format(acc, prec, rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b476eeb",
   "metadata": {},
   "source": [
    "Как мы видим, обе модели неплохо определяют принадлежность к одному из классов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ae99b",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef266a",
   "metadata": {},
   "source": [
    "DT или решающие деревья достаточно интересный классификатор, основной идеей которого является лучшее разделение по одному признаку в каждом узле дерева, так образуя разные области, которые принадлежат одному или другому классу. Лучшее разделения определяется с помощью неопределённонсти Джини(Gini impurity). Оно делается достаточно просто, с точки зрения понимания, мы просто перебираем все возможные разделения для текущего узла и выбираем то, для которого прибыль Джини (Gini gain) наибольшая. Так происходит обучение дерева. Дальше для определения принадлежности к одному или к другому классу мы просто спускаемся вниз по дереву как в бинарном дереве поиска, т.е. делая сравнение признака текущего узла с разделяющим его значением. В нашем случае мы идём влево, если наше значение меньше, и вправо иначе."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f5608",
   "metadata": {},
   "source": [
    "Gini impurity:\n",
    "$G(R) = 1 - \\sum_{i=1}^N P(x_i \\in R) $\n",
    "\n",
    "Gini gain:\n",
    "$G_{gain}(L,R)= G(L+R) - \\left( \\frac{|L|}{|L+R|} \\cdot G(L)+ \\frac{|R|}{|L+R|} \\cdot G(R)\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2c6ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick value count calculator\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Node: \n",
    "    \"\"\"\n",
    "    Class for creating the nodes for a decision tree \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        X: np.array,\n",
    "        Y: np.array,\n",
    "        min_samples_split=None,\n",
    "        max_depth=None,\n",
    "        depth=None\n",
    "    ):\n",
    "        # Saving the data to the node \n",
    "        self.Y = Y \n",
    "        self.X = X\n",
    "\n",
    "        # Saving the hyper parameters\n",
    "        self.min_samples_split = min_samples_split if min_samples_split else 20\n",
    "        self.max_depth = max_depth if max_depth else 5\n",
    "\n",
    "        # Default current depth of node \n",
    "        self.depth = depth if depth else 0\n",
    "\n",
    "        # Extracting all the features\n",
    "        self.features = range(self.X.shape[1])\n",
    "\n",
    "        # Calculating the counts of Y in the node \n",
    "        self.counts = Counter(Y)\n",
    "\n",
    "        # Getting the GINI impurity based on the Y distribution\n",
    "        self.gini_impurity = self.get_GINI()\n",
    "\n",
    "        # Sorting the counts and saving the final prediction of the node \n",
    "        counts_sorted = list(sorted(self.counts.items(), key=lambda item: item[1]))\n",
    "\n",
    "        # Getting the last item\n",
    "        yhat = None\n",
    "        if len(counts_sorted) > 0:\n",
    "            yhat = counts_sorted[-1][0]\n",
    "\n",
    "        # Saving to object attribute. This node will predict the class with the most frequent class\n",
    "        self.yhat = yhat \n",
    "\n",
    "        # Saving the number of observations in the node \n",
    "        self.n = len(Y)\n",
    "\n",
    "        # Initiating the left and right nodes as empty nodes\n",
    "        self.left = None \n",
    "        self.right = None \n",
    "\n",
    "        # Default values for splits\n",
    "        self.best_feature = None \n",
    "        self.best_value = None \n",
    "\n",
    "    @staticmethod\n",
    "    def GINI_impurity(y1_count: int, y2_count: int) -> float:\n",
    "        \"\"\"\n",
    "        Given the observations of a binary class calculate the GINI impurity\n",
    "        \"\"\"\n",
    "        # Ensuring the correct types\n",
    "        if y1_count is None:\n",
    "            y1_count = 0\n",
    "\n",
    "        if y2_count is None:\n",
    "            y2_count = 0\n",
    "\n",
    "        # Getting the total observations\n",
    "        n = y1_count + y2_count\n",
    "        \n",
    "        # If n is 0 then we return the lowest possible gini impurity\n",
    "        if n == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Getting the probability to see each of the classes\n",
    "        p1 = y1_count / n\n",
    "        p2 = y2_count / n\n",
    "        \n",
    "        # Calculating GINI \n",
    "        gini = 1 - (p1 ** 2 + p2 ** 2)\n",
    "        \n",
    "        # Returning the gini impurity\n",
    "        return gini\n",
    "\n",
    "    @staticmethod\n",
    "    def ma(x: np.array, window: int) -> np.array:\n",
    "        \"\"\"\n",
    "        Calculates the moving average of the given list. \n",
    "        \"\"\"\n",
    "        return np.convolve(x, np.ones(window), 'valid') / window\n",
    "\n",
    "    def get_GINI(self):\n",
    "        \"\"\"\n",
    "        Function to calculate the GINI impurity of a node \n",
    "        \"\"\"\n",
    "        # Getting the 0 and 1 counts\n",
    "        y1_count, y2_count = self.counts.get(0, 0), self.counts.get(1, 0)\n",
    "\n",
    "        # Getting the GINI impurity\n",
    "        return self.GINI_impurity(y1_count, y2_count)\n",
    "\n",
    "    def best_split(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Given the X features and Y targets calculates the best split \n",
    "        for a decision tree\n",
    "        \"\"\"\n",
    "        # Creating a dataset for spliting\n",
    "        df = pd.DataFrame(self.X)\n",
    "        df['Y'] = self.Y\n",
    "        \n",
    "        # Getting the GINI impurity for the base input \n",
    "        GINI_base = self.get_GINI()\n",
    "\n",
    "        # Finding which split yields the best GINI gain \n",
    "        max_gain = 0\n",
    "\n",
    "        # Default best feature and split\n",
    "        best_feature = None\n",
    "        best_value = None\n",
    "\n",
    "        for feature in self.features:\n",
    "            \n",
    "            Xdf = df.sort_values(feature)\n",
    "\n",
    "            # Sorting the values and getting the rolling average\n",
    "            xmeans = self.ma(Xdf[feature].unique(), 2)\n",
    "\n",
    "            for value in xmeans:\n",
    "                # Spliting the dataset \n",
    "                left_counts = Counter(Xdf[Xdf[feature]<value]['Y'])\n",
    "                right_counts = Counter(Xdf[Xdf[feature]>=value]['Y'])\n",
    "\n",
    "                # Getting the Y distribution from the dicts\n",
    "                y0_left, y1_left, y0_right, y1_right = left_counts.get(0, 0), left_counts.get(1, 0), right_counts.get(0, 0), right_counts.get(1, 0)\n",
    "\n",
    "                # Getting the left and right gini impurities\n",
    "                gini_left = self.GINI_impurity(y0_left, y1_left)\n",
    "                gini_right = self.GINI_impurity(y0_right, y1_right)\n",
    "\n",
    "                # Getting the obs count from the left and the right data splits\n",
    "                n_left = y0_left + y1_left\n",
    "                n_right = y0_right + y1_right\n",
    "\n",
    "                # Calculating the weights for each of the nodes\n",
    "                w_left = n_left / (n_left + n_right)\n",
    "                w_right = n_right / (n_left + n_right)\n",
    "\n",
    "                # Calculating the weighted GINI impurity\n",
    "                wGINI = w_left * gini_left + w_right * gini_right\n",
    "\n",
    "                # Calculating the GINI gain \n",
    "                GINIgain = GINI_base - wGINI\n",
    "\n",
    "                # Checking if this is the best split so far \n",
    "                if GINIgain > max_gain:\n",
    "                    best_feature = feature\n",
    "                    best_value = value \n",
    "\n",
    "                    # Setting the best gain to the current one \n",
    "                    max_gain = GINIgain\n",
    "\n",
    "        return (best_feature, best_value)\n",
    "\n",
    "    def grow_tree(self):\n",
    "        \"\"\"\n",
    "        Recursive method to create the decision tree\n",
    "        \"\"\"\n",
    "        # Making a df from the data \n",
    "        df = pd.DataFrame(self.X)\n",
    "        df['Y'] = self.Y\n",
    "\n",
    "        # If there is GINI to be gained, we split further \n",
    "        if (self.depth < self.max_depth) and (self.n >= self.min_samples_split):\n",
    "\n",
    "            # Getting the best split \n",
    "            best_feature, best_value = self.best_split()\n",
    "\n",
    "            if best_feature is not None:\n",
    "                # Saving the best split to the current node \n",
    "                self.best_feature = best_feature\n",
    "                self.best_value = best_value\n",
    "\n",
    "                # Getting the left and right nodes\n",
    "                left_df, right_df = df[df[best_feature]<=best_value].copy(), df[df[best_feature]>best_value].copy()\n",
    "\n",
    "                # Creating the left and right nodes\n",
    "                left = Node(\n",
    "                    left_df[self.features].to_numpy(), \n",
    "                    left_df['Y'].values, \n",
    "                    depth=self.depth + 1, \n",
    "                    max_depth=self.max_depth, \n",
    "                    min_samples_split=self.min_samples_split\n",
    "                    )\n",
    "\n",
    "                self.left = left \n",
    "                self.left.grow_tree()\n",
    "\n",
    "                right = Node(\n",
    "                    right_df[self.features].to_numpy(), \n",
    "                    right_df['Y'].values, \n",
    "                    depth=self.depth + 1, \n",
    "                    max_depth=self.max_depth, \n",
    "                    min_samples_split=self.min_samples_split\n",
    "                )\n",
    "\n",
    "                self.right = right\n",
    "                self.right.grow_tree()\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Batch prediction method\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        for x in X:\n",
    "            values = {}\n",
    "            for feature in self.features:\n",
    "                values.update({feature: x[feature]})\n",
    "        \n",
    "            predictions.append(self.predict_obs(values))\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict_obs(self, values: dict) -> int:\n",
    "        \"\"\"\n",
    "        Method to predict the class given a set of features\n",
    "        \"\"\"\n",
    "        cur_node = self\n",
    "        while cur_node.depth < cur_node.max_depth:\n",
    "            # Traversing the nodes all the way to the bottom\n",
    "            best_feature = cur_node.best_feature\n",
    "            best_value = cur_node.best_value\n",
    "\n",
    "            if cur_node.n < cur_node.min_samples_split:\n",
    "                break \n",
    "\n",
    "            if (values.get(best_feature) < best_value):\n",
    "                if self.left is not None:\n",
    "                    cur_node = cur_node.left\n",
    "            else:\n",
    "                if self.right is not None:\n",
    "                    cur_node = cur_node.right\n",
    "            \n",
    "        return cur_node.yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "350f20ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7987926230595728\n",
      "precision: 0.832078795643818\n",
      "recall:    0.917365586651364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initiating the Node\n",
    "root = Node(X_l, Y_l, max_depth=4, min_samples_split=1000)\n",
    "\n",
    "# Getting teh best split\n",
    "root.grow_tree()\n",
    "\n",
    "\n",
    "# Predicting \n",
    "Xsubset = X_t.copy()\n",
    "Y_pred = np.array(root.predict(Xsubset))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_t, Y_pred).ravel()\n",
    "\n",
    "acc:float = (tn + tp)/(tp+tn + fp + fn)\n",
    "prec:float = tp/(tp + fp)\n",
    "rec:float = tp/(tp + fn)\n",
    "print(\"accuracy:  {0}\\nprecision: {1}\\nrecall:    {2}\".format(acc, prec, rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f345883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7881783202865862\n",
      "precision: 0.8311889250814333\n",
      "recall:    0.901121214796504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X_l, Y_l)\n",
    "Y_pred = clf.predict(X_t)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_t, Y_pred).ravel()\n",
    "\n",
    "acc:float = (tn + tp)/(tp+tn + fp + fn)\n",
    "prec:float = tp/(tp + fp)\n",
    "rec:float = tp/(tp + fn)\n",
    "print(\"accuracy:  {0}\\nprecision: {1}\\nrecall:    {2}\".format(acc, prec, rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d76d86",
   "metadata": {},
   "source": [
    "Как мы видим, получившаяся модель даже несколько лучше, чем готовая из sklearn. Можно предположить, что у нас просто лучше подобраны гиперпараметры(максимальная глубина дерева и минимальный размер выборки, меньше которого мы не делим), из-за чего наше дерево меньше переобучилось, и получился более хороший результат."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e6cef",
   "metadata": {},
   "source": [
    "# Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc4eb2",
   "metadata": {},
   "source": [
    "В целом, данная работа научила меня работать с python и библиотеками numpy, pandas и sklearn гораздо лучше. Разобрал три алгоритма классификации Logistic Regression, SVM и Decision tree, написал их реализации и сравнил их работу с библиотечными."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
